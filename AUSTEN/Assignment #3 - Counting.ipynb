{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words in Austen\n",
    "\n",
    "I wanted to reproduce the counting values for words in the Austen corpus that I found in Voyant. First I experimented with finding words in a single text by opening the text, having it read and then counting the number of instances of \"Mr\" in that text. Note that the search is case sensitive but picks up \"Mr\" within other words (namely \"Mrs\" I imagine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LOVE AND FREINDSHIP AND OTHER EARLY WORKS\\n\\n(Love And Friendship And Other Early '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opening up the file, printing some of it to make sure it's working:\n",
    "with open (\"austen\\\\1790 Love And Freindship.txt\", \"r\") as f:\n",
    "    austen1790 = f.read()\n",
    "austen1790[0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of 'Mr' in Love and Freindship: 49\n"
     ]
    }
   ],
   "source": [
    "# Counting the instances of \"Mr\":\n",
    "print(\"Instances of 'Mr' in Love and Freindship:\", austen1790.count(\"Mr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "I wanted to search through all text files, however, so I opened them using glob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen\\\\1790 Love And Freindship.txt',\n",
       " 'austen\\\\1805 Lady Susan.txt',\n",
       " 'austen\\\\1811 Sense and Sensibility.txt',\n",
       " 'austen\\\\1813 Pride and Prejudice.txt',\n",
       " 'austen\\\\1814 Mansfield Park.txt',\n",
       " 'austen\\\\1815 Emma.txt',\n",
       " 'austen\\\\1818 Northanger Abbey.txt',\n",
       " 'austen\\\\1818 Persuasion.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "textFiles = glob.glob(\"austen\\\\*txt\")\n",
    "textFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the issue of picking up words within words, I turned to regular expressions.\n",
    "\n",
    "I wrote a loop to open the texts and convert the characters within them to upper case before searching for \"MR\" within them in case the texts had typos where \"Mr\" was not capitalized. I used a regular expression to search for the term. I opted to ask for no letter characters on either side of \"Mr\" ([^\\w]) to allow for the possibility of \"Mr\" appearing with or without a period. \n",
    "\n",
    "I was getting a codec error message saying that one of the bytes couldn't be properly decoded. An online search suggested I specify the encoding using encoding=\"utf-8\".\n",
    "\n",
    "I wanted to print the number of times \"Mr\" was used for each text, so I used the len function which indicated how many items were in the list that had been made from instances of \"Mr\". I wanted a final count of all the occurrences of the word as well, so used the += operator to add the count from each text to the overall count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen\\1790 Love And Freindship.txt : 25\n",
      "austen\\1805 Lady Susan.txt : 77\n",
      "austen\\1811 Sense and Sensibility.txt : 178\n",
      "austen\\1813 Pride and Prejudice.txt : 785\n",
      "austen\\1814 Mansfield Park.txt : 482\n",
      "austen\\1815 Emma.txt : 1153\n",
      "austen\\1818 Northanger Abbey.txt : 161\n",
      "austen\\1818 Persuasion.txt : 256\n",
      "total counts of Mr/Mr.:  3117\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "totalMr = 0 # This is the count\n",
    "for textFile in textFiles:\n",
    "    f = open(textFile, \"r\", encoding=\"utf-8\") # Needed to specify the encoding because I was getting a codec error\n",
    "    textString = f.read().upper() # converted text to upper case\n",
    "    f.close()\n",
    "    mr = re.findall(\"([^\\w]MR[^\\w])\", textString) # \"MR\" has to be in upper case to match the upper case text\n",
    "    mr2 = len(mr) # tells us how long the list \"mr\" is in order to give word counts\n",
    "    print(textFile, \":\", mr2) \n",
    "    totalMr += mr2 # adds the number of occurrences of \"Mr\" found in each string after each iteration to the total number of occurrences\n",
    "print(\"total counts of Mr/Mr.: \", totalMr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't want to have to go through all these steps for each word, so I wrote a function to do it for me. As you can see, the function does the same thing as above, simply replacing \"Mr\" with a variable called \"word\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCount(word):\n",
    "    totalWord = 0\n",
    "    for textFile in textFiles:\n",
    "        f = open(textFile, \"r\", encoding=\"utf-8\") \n",
    "        textString = f.read().upper()\n",
    "        f.close()\n",
    "        wordList = re.findall(\"[^\\w]\"+word.upper()+\"[^\\w]\", textString) #I added the instruction to capitalize the input word so I wouldn't have to remember to input a word in all caps every time\n",
    "        numWords = len(wordList)\n",
    "        print(textFile, \":\", numWords)\n",
    "        totalWord += numWords\n",
    "    print(\"Total count of\", word, \":\", totalWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could then search for any word I wanted to find the counts in each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen\\1790 Love And Freindship.txt : 24\n",
      "austen\\1805 Lady Susan.txt : 61\n",
      "austen\\1811 Sense and Sensibility.txt : 530\n",
      "austen\\1813 Pride and Prejudice.txt : 343\n",
      "austen\\1814 Mansfield Park.txt : 408\n",
      "austen\\1815 Emma.txt : 699\n",
      "austen\\1818 Northanger Abbey.txt : 175\n",
      "austen\\1818 Persuasion.txt : 291\n",
      "Total count of Mrs : 2531\n"
     ]
    }
   ],
   "source": [
    "getWordCount(\"Mrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't actually need to know the word counts for the individual texts, so I then altered the function to only include the total count for all the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCount(word):\n",
    "    totalWord = 0\n",
    "    for textFile in textFiles:\n",
    "        f = open(textFile, \"r\", encoding=\"utf-8\") \n",
    "        textString = f.read().upper()\n",
    "        f.close()\n",
    "        wordList = re.findall(\"[^\\w]\"+word.upper()+\"[^\\w]\", textString)\n",
    "        numWords = len(wordList) # removed instructions to print text files + counts after this line\n",
    "        totalWord += numWords \n",
    "    return totalWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I decided to simply make a list of the words I wanted to search for, and using a loop apply each word to my function, ending up with a list of how many times each word appeared in my corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr 3117\n",
      "mrs 2531\n",
      "said 2165\n",
      "miss 1942\n",
      "think 1514\n",
      "know 1450\n",
      "good 1444\n",
      "time 1432\n",
      "little 1363\n",
      "soon 1124\n",
      "say 1109\n",
      "great 1053\n",
      "lady 1042\n",
      "dear 960\n",
      "shall 938\n",
      "sir 921\n",
      "quite 904\n",
      "man 928\n",
      "thought 866\n",
      "fanny 980\n"
     ]
    }
   ],
   "source": [
    "wordList = [\"mr\", \"mrs\", \"said\", \"miss\", \"think\", \"know\", \"good\", \"time\", \"little\", \"soon\", \"say\", \"great\", \"lady\", \"dear\", \"shall\", \"sir\", \"quite\", \"man\", \"thought\", \"fanny\"]\n",
    "for word in wordList:\n",
    "    count = getWordCount(word)\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between my word counts and those in Voyant is that because I used the regular expression [^\\w]\"+word+\"[^\\w], I am picking up instances when the word is followed by an apostrophe (denoting possession), so my counts of \"lady,\" \"man,\" and \"fanny\" are all higher than those in Voyant which counts \"lady\" and \"lady's\" as two separate words. I prefer picking up both in my search, but if I wanted to match Voyant more exactly, I could change my regular expression to read [^\\w]\"+word+\"[^'a-z'] (a non-letter character, the word, then no apostrophe or letters). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lady 1012\n",
      "man 887\n",
      "fanny 864\n"
     ]
    }
   ],
   "source": [
    "def getWordCount2(word):\n",
    "    totalWord = 0\n",
    "    for textFile in textFiles:\n",
    "        f = open(textFile, \"r\", encoding=\"utf-8\") \n",
    "        textString = f.read().upper()\n",
    "        f.close()\n",
    "        wordList = re.findall(\"[^\\w]\"+word.upper()+\"[^'A-Z']\", textString)\n",
    "        numWords = len(wordList) \n",
    "        totalWord += numWords \n",
    "    return totalWord\n",
    "\n",
    "wordList2 = [\"lady\", \"man\", \"fanny\"]\n",
    "for word in wordList2:\n",
    "    count = getWordCount2(word)\n",
    "    print(word, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even so, I am mysteriously off by one for both \"lady\" and \"fanny\" which I couldn't figure out. I decided try to find adverbs in one of the texts to take my mind off my frustration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Early',\n",
       " 'comply',\n",
       " 'Surely',\n",
       " 'surely',\n",
       " 'considerably',\n",
       " 'lovely',\n",
       " 'shortly',\n",
       " 'tremblingly']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adverbs = re.findall(r\"\\w+ly\", austen1790) # austen1790 was defined above (refers to \"1790 Love And Freindship.txt\")\n",
    "adverbs[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Tremblingly\" was my favourite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
